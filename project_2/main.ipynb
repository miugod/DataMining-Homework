{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 使用第三张GPU卡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "196\n",
      "198607\n",
      "198607\n",
      "198607\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from pyjarowinkler import distance\n",
    "import numpy as np\n",
    "train=True\n",
    "# 训练集中的作者论文信息\n",
    "with open(\"train/train_author.json\", \"r\") as f2:\n",
    "    author_data = json.load(f2)\n",
    "\n",
    "# 训练集的论文元信息\n",
    "with open(\"train/train_pub.json\", \"r\") as f2:\n",
    "    pubs_dict = json.load(f2)\n",
    "\n",
    "print(len(author_data))\n",
    "\n",
    "\n",
    "name_train = set()\n",
    "\n",
    "# 筛选训练集，只取同名作者数大于等于5个的名字作为训练集。\n",
    "for name in author_data:\n",
    "    persons = author_data[name]\n",
    "    if(len(persons) > 5):\n",
    "        name_train.add((name))\n",
    "\n",
    "print(len(name_train))\n",
    "\n",
    "# 采样500个训练例子，一个训练例子包含paper和正例作者以及5个负例作者（正负例比=1：5）\n",
    "\n",
    "# 记录paper所属作者和名字\n",
    "paper2aid2name = {}\n",
    "\n",
    "for author_name in name_train:\n",
    "    persons = author_data[author_name]\n",
    "    for person in persons:\n",
    "        paper_list = persons[person]\n",
    "        for paper_id in paper_list:\n",
    "            paper2aid2name[paper_id] = (author_name, person)\n",
    "\n",
    "print(len(paper2aid2name))\n",
    "# print(paper2aid2name)\n",
    "\n",
    "total_paper_list = list(paper2aid2name.keys())\n",
    "\n",
    "# 采样10000篇paper作为训练集\n",
    "print(len(total_paper_list))\n",
    "# train_paper_list = random.sample(total_paper_list, 40000)  # comment it to train all\n",
    "train_paper_list = total_paper_list\n",
    "\n",
    "# train_paper_list = []\n",
    "    \n",
    "# 把采样的500篇paper转变成对应的训练例子，一个训练例子包含paper和正例作者以及5个负例作者（正负例比=1：5）\n",
    "train_instances = []\n",
    "for paper_id in train_paper_list:\n",
    "    \n",
    "    # 保存对应的正负例\n",
    "    pos_ins = set()\n",
    "    neg_ins = set()\n",
    "    \n",
    "    paper_author_name = paper2aid2name[paper_id][0]\n",
    "    paper_author_id = paper2aid2name[paper_id][1]\n",
    "    \n",
    "    pos_ins.add((paper_id, paper_author_id))\n",
    "    \n",
    "    # 获取同名的所有作者(除了本身)作为负例的candidate\n",
    "    persons = list(author_data[paper_author_name].keys())\n",
    "    persons.remove(paper_author_id)\n",
    "    assert len(persons) == (len(list(author_data[paper_author_name].keys())) - 1)\n",
    "    \n",
    "    # 每个正例采样5个负例\n",
    "    neg_author_list = random.sample(persons, 5)\n",
    "    for i in neg_author_list:\n",
    "        neg_ins.add((paper_id, i))\n",
    "        \n",
    "    train_instances.append((pos_ins, neg_ins))\n",
    "    \n",
    "print(len(train_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyjarowinkler import distance\n",
    "\n",
    "\n",
    "# 对author_name 进行清洗\n",
    "def clean_name(name):\n",
    "    if name is None:\n",
    "        return \"\"\n",
    "    x = [k.strip() for k in name.lower().strip().replace(\".\", \"\").replace(\"-\", \" \").replace(\"_\", ' ').split()]\n",
    "    # x = [k.strip() for k in name.lower().strip().replace(\"-\", \"\").replace(\"_\", ' ').split()]\n",
    "    full_name = ' '.join(x)\n",
    "    name_part = full_name.split()\n",
    "    if(len(name_part) >= 1):\n",
    "        return full_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 找出paper中author_name所对应的位置\n",
    "def delete_main_name(author_list, name):\n",
    "    score_list = []\n",
    "    name = clean_name(name)\n",
    "    author_list_lower = []\n",
    "    for author in author_list:\n",
    "        author_list_lower.append(author.lower())\n",
    "    name_split = name.split()\n",
    "    for author in author_list_lower:\n",
    "        # lower_name = author.lower()\n",
    "        score = distance.get_jaro_distance(name, author, winkler=True, scaling=0.1)\n",
    "        author_split = author.split()\n",
    "        inter = set(name_split) & set(author_split)\n",
    "        alls = set(name_split) | set(author_split)\n",
    "        score += round(len(inter)/len(alls), 6)\n",
    "        score_list.append(score)\n",
    "\n",
    "    rank = np.argsort(-np.array(score_list))\n",
    "    return_list = [author_list_lower[i] for i in rank[1:]]\n",
    "\n",
    "    return return_list, rank[0]\n",
    "\n",
    "# 训练集特征生成函数\n",
    "def process_feature_coauthor(pos_ins, paper_coauthors):\n",
    "    \n",
    "    feature_list = []\n",
    "\n",
    "    paper = pos_ins[0] \n",
    "    author = pos_ins[1]\n",
    "\n",
    "\n",
    "    paper_name = paper2aid2name[paper][0]\n",
    "    \n",
    "    # 从作者的论文列表中把该篇论文去掉，防止训练出现bias\n",
    "    doc_list = []\n",
    "    for doc in author_data[paper_name][author]:\n",
    "        if(doc != paper):\n",
    "            doc_list.append(doc)\n",
    "    for doc in doc_list:\n",
    "        if doc == paper:\n",
    "            print(\"error!\")\n",
    "            exit()\n",
    "    \n",
    "    # 保存作者的所有paper的coauthors以及各自出现的次数(作者所拥有论文的coauthors)\n",
    "    candidate_authors_int = defaultdict(int)\n",
    "\n",
    "    total_author_count = 0\n",
    "    for doc in doc_list:\n",
    "        \n",
    "        doc_dict = pubs_dict[doc]\n",
    "        author_list = []\n",
    "\n",
    "        paper_authors = doc_dict['authors']\n",
    "        paper_authors_len = len(paper_authors)\n",
    "#         paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "    \n",
    "        for author in paper_authors:                \n",
    "            clean_author = clean_name(author['name'])\n",
    "            if(clean_author != None):\n",
    "                author_list.append(clean_author)\n",
    "        if(len(author_list) > 0):\n",
    "            # 获取paper中main author_name所对应的位置\n",
    "            _, author_index = delete_main_name(author_list, paper_name)\n",
    "\n",
    "            # 获取除了main author_name外的coauthor\n",
    "            for index in range(len(author_list)):\n",
    "                if(index == author_index):\n",
    "                    continue\n",
    "                else:\n",
    "                    candidate_authors_int[author_list[index]] += 1\n",
    "                    total_author_count += 1\n",
    "\n",
    "    # author 的所有不同coauthor name\n",
    "    author_keys = list(candidate_authors_int.keys())\n",
    "\n",
    "    if ((len(author_keys) == 0) or (len(paper_coauthors) == 0)):\n",
    "        feature_list.extend([0.] * 5)\n",
    "    else:\n",
    "        co_coauthors = set(paper_coauthors) & set(author_keys)\n",
    "        coauthor_len = len(co_coauthors)\n",
    "        \n",
    "        \n",
    "        co_coauthors_ratio_for_paper = round(coauthor_len / len(paper_coauthors), 6)\n",
    "        co_coauthors_ratio_for_author = round(coauthor_len / len(author_keys), 6)\n",
    "        \n",
    "        coauthor_count = 0\n",
    "        for coauthor_name in co_coauthors:\n",
    "            coauthor_count += candidate_authors_int[coauthor_name]\n",
    "            \n",
    "        \n",
    "        \n",
    "        co_coauthors_ratio_for_author_count = round(coauthor_count / total_author_count, 6)\n",
    "\n",
    "        # 计算了5维paper与author所有的paper的coauthor相关的特征：\n",
    "        #    1. 不重复的coauthor个数\n",
    "        #    2. 不重复的coauthor个数 / paper的所有coauthor的个数\n",
    "        #    3. 不重复的coauthor个数 / author的所有paper不重复coauthor的个数\n",
    "        #    4. coauthor个数（含重复）\n",
    "        #    4. coauthor个数（含重复）/ author的所有paper的coauthor的个数（含重复）\n",
    "        feature_list.extend([coauthor_len, co_coauthors_ratio_for_paper, co_coauthors_ratio_for_author, coauthor_count, co_coauthors_ratio_for_author_count])\n",
    "        \n",
    "#         print(feature_list)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_keywords(pos_ins, topic_list):\n",
    "    \n",
    "    feature_list = []\n",
    "\n",
    "    paper = pos_ins[0] \n",
    "    author = pos_ins[1]\n",
    "\n",
    "\n",
    "    paper_name = paper2aid2name[paper][0]\n",
    "    \n",
    "    # 从作者的论文列表中把该篇论文去掉，防止训练出现bias\n",
    "    doc_list = []\n",
    "    for doc in author_data[paper_name][author]:\n",
    "        if(doc != paper):\n",
    "            doc_list.append(doc)\n",
    "    for doc in doc_list:\n",
    "        if doc == paper:\n",
    "            print(\"error!\")\n",
    "            exit()\n",
    "    \n",
    "    # 保存作者的所有paper的coauthors以及各自出现的次数(作者所拥有论文的coauthors)\n",
    "    candidate_keywords_int = defaultdict(int)\n",
    "\n",
    "    total_keyword_count = 0\n",
    "    for doc in doc_list:\n",
    "        \n",
    "        doc_dict = pubs_dict[doc]\n",
    "        keyword_list = []\n",
    "        if ('keywords' not in doc_dict.keys()):\n",
    "            feature_list.extend([0.] * 5)\n",
    "            return feature_list\n",
    "        paper_keywords = doc_dict['keywords']\n",
    "        paper_keywords_len = len(paper_keywords)\n",
    "        paper_keywords = random.sample(paper_keywords, min(10, paper_keywords_len))\n",
    "        \n",
    "        for keywords in paper_keywords:\n",
    "            clean_keywords=clean_name(keywords)\n",
    "            if clean_keywords!=None:\n",
    "                keyword_list.append(clean_keywords)\n",
    "        if(len(keyword_list) > 0):\n",
    "            # 获取除了main author_name外的coauthor\n",
    "            for index in range(len(keyword_list)):\n",
    "                candidate_keywords_int[keyword_list[index]] += 1\n",
    "                total_keyword_count += 1\n",
    "\n",
    "    # author 的所有不同coauthor name\n",
    "    author_keys = list(candidate_keywords_int.keys())\n",
    "\n",
    "    if ((len(author_keys) == 0) or (len(paper_keywords) == 0)):\n",
    "        feature_list.extend([0.] * 5)\n",
    "    else:\n",
    "        co_keywords = set(paper_keywords) & set(author_keys)\n",
    "        keyword_len = len(co_keywords)\n",
    "#         same=[x for x in paper_keywords if  x in author_keys]\n",
    "        co_keywords_ratio_for_paper = round(keyword_len / len(paper_keywords), 6)\n",
    "        co_keywords_ratio_for_author = round(keyword_len / len(author_keys), 6)\n",
    "        \n",
    "        keywords_count = 0\n",
    "        for keywords_name in co_keywords:\n",
    "            keywords_count += candidate_keywords_int[keywords_name]\n",
    "            \n",
    "        \n",
    "        \n",
    "        co_keywords_ratio_for_author_count = round(keywords_count / total_keyword_count, 6)\n",
    "\n",
    "        \n",
    "        feature_list.extend([keyword_len, co_keywords_ratio_for_paper, co_keywords_ratio_for_paper, keywords_count, co_keywords_ratio_for_author_count])\n",
    "        \n",
    "#         print(feature_list)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/198607 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 3/198607 [00:00<2:19:26, 23.74it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/198607 [00:00<2:06:41, 26.12it/s]\u001b[A\n",
      "  0%|          | 11/198607 [00:00<2:00:53, 27.38it/s]\u001b[A\n",
      "  0%|          | 15/198607 [00:00<1:56:53, 28.32it/s]\u001b[A\n",
      "  0%|          | 19/198607 [00:00<1:49:15, 30.29it/s]\u001b[A\n",
      "  0%|          | 22/198607 [00:00<1:52:19, 29.47it/s]\u001b[A\n",
      "  0%|          | 26/198607 [00:00<1:49:57, 30.10it/s]\u001b[A\n",
      "  0%|          | 30/198607 [00:00<1:45:19, 31.42it/s]\u001b[A\n",
      "  0%|          | 34/198607 [00:01<1:44:44, 31.60it/s]\u001b[A\n",
      "  0%|          | 38/198607 [00:01<1:46:28, 31.08it/s]\u001b[A\n",
      "  0%|          | 42/198607 [00:01<1:45:27, 31.38it/s]\u001b[A\n",
      "  0%|          | 46/198607 [00:01<1:46:47, 30.99it/s]\u001b[A\n",
      "  0%|          | 50/198607 [00:01<1:41:36, 32.57it/s]\u001b[A\n",
      "  0%|          | 54/198607 [00:01<1:41:31, 32.60it/s]\u001b[A\n",
      "  0%|          | 58/198607 [00:01<1:43:43, 31.90it/s]\u001b[A\n",
      "  0%|          | 62/198607 [00:01<1:44:36, 31.63it/s]\u001b[A\n",
      "  0%|          | 66/198607 [00:02<1:46:10, 31.17it/s]\u001b[A\n",
      "  0%|          | 70/198607 [00:02<1:43:40, 31.91it/s]\u001b[A\n",
      "  0%|          | 74/198607 [00:02<1:44:07, 31.78it/s]\u001b[A\n",
      "  0%|          | 78/198607 [00:02<1:45:29, 31.36it/s]\u001b[A\n",
      "  0%|          | 82/198607 [00:02<1:44:59, 31.52it/s]\u001b[A\n",
      "  0%|          | 86/198607 [00:02<1:46:07, 31.18it/s]\u001b[A\n",
      "  0%|          | 90/198607 [00:02<1:48:06, 30.61it/s]\u001b[A\n",
      "  0%|          | 94/198607 [00:03<1:51:02, 29.79it/s]\u001b[A\n",
      "  0%|          | 98/198607 [00:03<1:44:50, 31.56it/s]\u001b[A\n",
      "  0%|          | 102/198607 [00:03<1:46:32, 31.05it/s]\u001b[A\n",
      "  0%|          | 107/198607 [00:03<1:35:11, 34.75it/s]\u001b[A\n",
      "  0%|          | 111/198607 [00:03<1:33:13, 35.49it/s]\u001b[A\n",
      "  0%|          | 115/198607 [00:03<1:35:23, 34.68it/s]\u001b[A\n",
      "  0%|          | 119/198607 [00:03<1:37:23, 33.97it/s]\u001b[A\n",
      "  0%|          | 124/198607 [00:03<1:32:05, 35.92it/s]\u001b[A\n",
      "  0%|          | 128/198607 [00:03<1:34:22, 35.05it/s]\u001b[A\n",
      "  0%|          | 133/198607 [00:04<1:29:46, 36.85it/s]\u001b[A\n",
      "  0%|          | 138/198607 [00:04<1:23:20, 39.69it/s]\u001b[A\n",
      "  0%|          | 143/198607 [00:04<1:27:32, 37.78it/s]\u001b[A\n",
      "  0%|          | 148/198607 [00:04<1:26:43, 38.14it/s]\u001b[A\n",
      "  0%|          | 152/198607 [00:04<1:30:18, 36.63it/s]\u001b[A\n",
      "  0%|          | 156/198607 [00:04<1:30:01, 36.74it/s]\u001b[A\n",
      "  0%|          | 160/198607 [00:04<1:34:54, 34.85it/s]\u001b[A\n",
      "  0%|          | 164/198607 [00:04<1:33:13, 35.48it/s]\u001b[A\n",
      "  0%|          | 168/198607 [00:05<1:30:43, 36.46it/s]\u001b[A\n",
      "  0%|          | 173/198607 [00:05<1:29:13, 37.07it/s]\u001b[A\n",
      "  0%|          | 178/198607 [00:05<1:25:10, 38.82it/s]\u001b[A\n",
      "  0%|          | 183/198607 [00:05<1:22:18, 40.18it/s]\u001b[A\n",
      "  0%|          | 188/198607 [00:05<1:19:14, 41.73it/s]\u001b[A\n",
      "  0%|          | 193/198607 [00:05<1:23:25, 39.64it/s]\u001b[A\n",
      "  0%|          | 198/198607 [00:05<1:31:01, 36.33it/s]\u001b[A\n",
      "  0%|          | 202/198607 [00:06<2:11:11, 25.21it/s]\u001b[A\n",
      "  0%|          | 206/198607 [00:06<2:40:12, 20.64it/s]\u001b[A\n",
      "  0%|          | 209/198607 [00:06<3:06:17, 17.75it/s]\u001b[A\n",
      "  0%|          | 212/198607 [00:06<3:17:19, 16.76it/s]\u001b[A\n",
      "  0%|          | 214/198607 [00:06<3:27:44, 15.92it/s]\u001b[A\n",
      "  0%|          | 216/198607 [00:07<3:34:24, 15.42it/s]\u001b[A\n",
      "  0%|          | 218/198607 [00:07<3:38:36, 15.13it/s]\u001b[A\n",
      "  0%|          | 220/198607 [00:07<3:40:16, 15.01it/s]\u001b[A\n",
      "  0%|          | 222/198607 [00:07<3:43:09, 14.82it/s]\u001b[A\n",
      "  0%|          | 224/198607 [00:07<3:47:53, 14.51it/s]\u001b[A\n",
      "  0%|          | 226/198607 [00:07<3:50:04, 14.37it/s]\u001b[A\n",
      "  0%|          | 228/198607 [00:07<3:48:07, 14.49it/s]\u001b[A\n",
      "  0%|          | 230/198607 [00:08<3:50:10, 14.36it/s]\u001b[A\n",
      "  0%|          | 232/198607 [00:08<3:52:07, 14.24it/s]\u001b[A\n",
      "  0%|          | 234/198607 [00:08<3:51:07, 14.30it/s]\u001b[A\n",
      "  0%|          | 236/198607 [00:08<3:51:29, 14.28it/s]\u001b[A\n",
      "  0%|          | 238/198607 [00:08<3:52:58, 14.19it/s]\u001b[A\n",
      "  0%|          | 580/198607 [00:50<3:52:37, 14.19it/s]\u001b[A\n",
      "  0%|          | 242/198607 [00:08<4:07:49, 13.34it/s]\u001b[A\n",
      "  0%|          | 244/198607 [00:09<4:05:58, 13.44it/s]\u001b[A\n",
      "  0%|          | 246/198607 [00:09<4:00:36, 13.74it/s]\u001b[A\n",
      "  0%|          | 248/198607 [00:09<3:57:07, 13.94it/s]\u001b[A\n",
      "  0%|          | 250/198607 [00:09<3:53:26, 14.16it/s]\u001b[A\n",
      "  0%|          | 252/198607 [00:09<3:53:21, 14.17it/s]\u001b[A\n",
      "  0%|          | 254/198607 [00:09<3:52:57, 14.19it/s]\u001b[A\n",
      "  0%|          | 256/198607 [00:09<3:54:33, 14.09it/s]\u001b[A\n",
      "  0%|          | 258/198607 [00:10<3:54:41, 14.09it/s]\u001b[A\n",
      "  0%|          | 260/198607 [00:10<3:55:46, 14.02it/s]\u001b[A\n",
      "  0%|          | 262/198607 [00:10<3:53:56, 14.13it/s]\u001b[A\n",
      "  0%|          | 264/198607 [00:10<3:54:01, 14.13it/s]\u001b[A\n",
      "  0%|          | 266/198607 [00:10<3:54:28, 14.10it/s]\u001b[A\n",
      "  0%|          | 268/198607 [00:10<3:52:51, 14.20it/s]\u001b[A\n",
      "  0%|          | 270/198607 [00:10<4:00:34, 13.74it/s]\u001b[A\n",
      "  0%|          | 272/198607 [00:11<3:54:40, 14.09it/s]\u001b[A\n",
      "  0%|          | 274/198607 [00:11<3:53:19, 14.17it/s]\u001b[A\n",
      "  0%|          | 276/198607 [00:11<3:49:06, 14.43it/s]\u001b[A\n",
      "  0%|          | 278/198607 [00:11<3:48:46, 14.45it/s]\u001b[A\n",
      "  0%|          | 280/198607 [00:11<3:51:18, 14.29it/s]\u001b[A\n",
      "  0%|          | 282/198607 [00:11<3:53:06, 14.18it/s]\u001b[A\n",
      "  0%|          | 284/198607 [00:11<3:54:01, 14.12it/s]\u001b[A\n",
      "  0%|          | 286/198607 [00:12<4:04:16, 13.53it/s]\u001b[A\n",
      "  0%|          | 288/198607 [00:12<4:00:31, 13.74it/s]\u001b[A\n",
      "  0%|          | 290/198607 [00:12<3:59:56, 13.78it/s]\u001b[A\n",
      "  0%|          | 292/198607 [00:12<3:57:52, 13.89it/s]\u001b[A\n",
      "  0%|          | 294/198607 [00:12<3:56:55, 13.95it/s]\u001b[A\n",
      "  0%|          | 296/198607 [00:12<3:57:08, 13.94it/s]\u001b[A\n",
      "  0%|          | 298/198607 [00:12<3:57:10, 13.94it/s]\u001b[A\n",
      "  0%|          | 300/198607 [00:13<3:57:27, 13.92it/s]\u001b[A\n",
      "  0%|          | 302/198607 [00:13<3:56:32, 13.97it/s]\u001b[A\n",
      "  0%|          | 304/198607 [00:13<3:54:03, 14.12it/s]\u001b[A\n",
      "  0%|          | 306/198607 [00:13<3:52:58, 14.19it/s]\u001b[A\n",
      "  0%|          | 308/198607 [00:13<3:54:12, 14.11it/s]\u001b[A\n",
      "  0%|          | 310/198607 [00:13<3:53:48, 14.14it/s]\u001b[A\n",
      "  0%|          | 312/198607 [00:13<3:51:07, 14.30it/s]\u001b[A\n",
      "  0%|          | 314/198607 [00:14<3:52:53, 14.19it/s]\u001b[A\n",
      "  0%|          | 316/198607 [00:14<4:01:34, 13.68it/s]\u001b[A\n",
      "  0%|          | 318/198607 [00:14<3:58:57, 13.83it/s]\u001b[A\n",
      "  0%|          | 320/198607 [00:14<3:54:49, 14.07it/s]\u001b[A\n",
      "  0%|          | 322/198607 [00:14<3:56:11, 13.99it/s]\u001b[A\n",
      "  0%|          | 324/198607 [00:14<3:55:51, 14.01it/s]\u001b[A\n",
      "  0%|          | 326/198607 [00:14<3:55:07, 14.05it/s]\u001b[A\n",
      "  0%|          | 328/198607 [00:15<3:51:42, 14.26it/s]\u001b[A\n",
      "  0%|          | 330/198607 [00:15<3:53:41, 14.14it/s]\u001b[A\n",
      "  0%|          | 332/198607 [00:15<4:00:07, 13.76it/s]\u001b[A\n",
      "  0%|          | 334/198607 [00:15<3:56:33, 13.97it/s]\u001b[A\n",
      "  0%|          | 336/198607 [00:15<3:55:45, 14.02it/s]\u001b[A\n",
      "  0%|          | 338/198607 [00:15<3:56:28, 13.97it/s]\u001b[A\n",
      "  0%|          | 340/198607 [00:15<3:56:30, 13.97it/s]\u001b[A\n",
      "  0%|          | 342/198607 [00:16<3:57:05, 13.94it/s]\u001b[A\n",
      "  0%|          | 344/198607 [00:16<3:55:43, 14.02it/s]\u001b[A\n",
      "  0%|          | 346/198607 [00:16<3:53:42, 14.14it/s]\u001b[A\n",
      "  0%|          | 348/198607 [00:16<3:58:52, 13.83it/s]\u001b[A\n",
      "  0%|          | 350/198607 [00:16<4:01:09, 13.70it/s]\u001b[A\n",
      "  0%|          | 352/198607 [00:16<3:59:44, 13.78it/s]\u001b[A\n",
      "  0%|          | 354/198607 [00:16<3:58:43, 13.84it/s]\u001b[A\n",
      "  0%|          | 356/198607 [00:17<3:57:09, 13.93it/s]\u001b[A\n",
      "  0%|          | 358/198607 [00:17<3:56:05, 14.00it/s]\u001b[A\n",
      "  0%|          | 360/198607 [00:17<3:54:12, 14.11it/s]\u001b[A\n",
      "  0%|          | 362/198607 [00:17<3:53:52, 14.13it/s]\u001b[A\n",
      "  0%|          | 364/198607 [00:17<3:59:55, 13.77it/s]\u001b[A\n",
      "  0%|          | 366/198607 [00:17<3:59:38, 13.79it/s]\u001b[A\n",
      "  0%|          | 368/198607 [00:17<3:55:25, 14.03it/s]\u001b[A\n",
      "  0%|          | 370/198607 [00:18<3:56:23, 13.98it/s]\u001b[A\n",
      "  0%|          | 372/198607 [00:18<3:54:30, 14.09it/s]\u001b[A\n",
      "  0%|          | 374/198607 [00:18<3:50:32, 14.33it/s]\u001b[A\n",
      "  0%|          | 376/198607 [00:18<3:52:31, 14.21it/s]\u001b[A\n",
      "  0%|          | 378/198607 [00:18<3:53:12, 14.17it/s]\u001b[A\n",
      "  0%|          | 380/198607 [00:18<3:59:49, 13.78it/s]\u001b[A\n",
      "  0%|          | 382/198607 [00:18<3:54:52, 14.07it/s]\u001b[A\n",
      "  0%|          | 384/198607 [00:19<4:02:06, 13.65it/s]\u001b[A\n",
      "  0%|          | 386/198607 [00:19<4:08:33, 13.29it/s]\u001b[A\n",
      "  0%|          | 388/198607 [00:19<4:03:29, 13.57it/s]\u001b[A\n",
      "  0%|          | 390/198607 [00:19<3:59:02, 13.82it/s]\u001b[A\n",
      "  0%|          | 392/198607 [00:19<3:58:07, 13.87it/s]\u001b[A\n",
      "  0%|          | 394/198607 [00:19<3:58:45, 13.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 396/198607 [00:19<3:54:57, 14.06it/s]\u001b[A\n",
      "  0%|          | 398/198607 [00:20<3:52:37, 14.20it/s]\u001b[A\n",
      "  0%|          | 400/198607 [00:20<3:52:58, 14.18it/s]\u001b[A\n",
      "  0%|          | 402/198607 [00:20<3:50:51, 14.31it/s]\u001b[A\n",
      "  0%|          | 404/198607 [00:20<3:50:28, 14.33it/s]\u001b[A\n",
      "  0%|          | 406/198607 [00:20<3:53:42, 14.13it/s]\u001b[A\n",
      "  0%|          | 408/198607 [00:20<3:55:08, 14.05it/s]\u001b[A\n",
      "  0%|          | 410/198607 [00:20<4:02:01, 13.65it/s]\u001b[A\n",
      "  0%|          | 412/198607 [00:21<4:01:32, 13.68it/s]\u001b[A\n",
      "  0%|          | 414/198607 [00:21<3:57:39, 13.90it/s]\u001b[A\n",
      "  0%|          | 416/198607 [00:21<3:57:37, 13.90it/s]\u001b[A\n",
      "  0%|          | 418/198607 [00:21<3:54:38, 14.08it/s]\u001b[A\n",
      "  0%|          | 420/198607 [00:21<3:53:21, 14.15it/s]\u001b[A\n",
      "  0%|          | 422/198607 [00:21<3:54:56, 14.06it/s]\u001b[A\n",
      "  0%|          | 424/198607 [00:21<3:54:52, 14.06it/s]\u001b[A\n",
      "  0%|          | 427/198607 [00:22<3:43:09, 14.80it/s]\u001b[A\n",
      "  0%|          | 429/198607 [00:22<3:44:38, 14.70it/s]\u001b[A\n",
      "  0%|          | 431/198607 [00:22<3:47:56, 14.49it/s]\u001b[A\n",
      "  0%|          | 433/198607 [00:22<3:49:38, 14.38it/s]\u001b[A\n",
      "  0%|          | 435/198607 [00:22<4:08:10, 13.31it/s]\u001b[A\n",
      "  0%|          | 437/198607 [00:22<4:05:58, 13.43it/s]\u001b[A\n",
      "  0%|          | 439/198607 [00:22<4:01:19, 13.69it/s]\u001b[A\n",
      "  0%|          | 441/198607 [00:23<4:02:17, 13.63it/s]\u001b[A\n",
      "  0%|          | 443/198607 [00:23<3:58:05, 13.87it/s]\u001b[A\n",
      "  0%|          | 445/198607 [00:23<3:54:46, 14.07it/s]\u001b[A\n",
      "  0%|          | 447/198607 [00:23<3:50:49, 14.31it/s]\u001b[A\n",
      "  0%|          | 449/198607 [00:23<3:50:09, 14.35it/s]\u001b[A\n",
      "  0%|          | 451/198607 [00:23<3:51:28, 14.27it/s]\u001b[A\n",
      "  0%|          | 453/198607 [00:23<3:51:17, 14.28it/s]\u001b[A\n",
      "  0%|          | 455/198607 [00:24<3:51:39, 14.26it/s]\u001b[A\n",
      "  0%|          | 457/198607 [00:24<3:58:20, 13.86it/s]\u001b[A\n",
      "  0%|          | 459/198607 [00:24<3:59:19, 13.80it/s]\u001b[A\n",
      "  0%|          | 461/198607 [00:24<4:02:24, 13.62it/s]\u001b[A\n",
      "  0%|          | 463/198607 [00:24<3:58:52, 13.82it/s]\u001b[A\n",
      "  0%|          | 465/198607 [00:24<3:58:29, 13.85it/s]\u001b[A\n",
      "  0%|          | 467/198607 [00:24<3:59:50, 13.77it/s]\u001b[A\n",
      "  0%|          | 469/198607 [00:25<3:56:10, 13.98it/s]\u001b[A\n",
      "  0%|          | 471/198607 [00:25<3:56:33, 13.96it/s]\u001b[A\n",
      "  0%|          | 473/198607 [00:25<3:56:12, 13.98it/s]\u001b[A\n",
      "  0%|          | 475/198607 [00:25<3:54:18, 14.09it/s]\u001b[A\n",
      "  0%|          | 477/198607 [00:25<3:55:36, 14.02it/s]\u001b[A\n",
      "  0%|          | 479/198607 [00:25<3:56:23, 13.97it/s]\u001b[A\n",
      "  0%|          | 481/198607 [00:25<3:56:07, 13.98it/s]\u001b[A\n",
      "  0%|          | 483/198607 [00:26<3:56:21, 13.97it/s]\u001b[A\n",
      "  0%|          | 485/198607 [00:26<3:53:50, 14.12it/s]\u001b[A\n",
      "  0%|          | 487/198607 [00:26<4:02:10, 13.63it/s]\u001b[A\n",
      "  0%|          | 489/198607 [00:26<3:58:22, 13.85it/s]\u001b[A\n",
      "  0%|          | 491/198607 [00:26<3:57:07, 13.92it/s]\u001b[A\n",
      "  0%|          | 493/198607 [00:26<3:58:02, 13.87it/s]\u001b[A\n",
      "  0%|          | 495/198607 [00:26<3:55:50, 14.00it/s]\u001b[A\n",
      "  0%|          | 497/198607 [00:27<3:59:01, 13.81it/s]\u001b[A\n",
      "  0%|          | 499/198607 [00:27<3:54:35, 14.07it/s]\u001b[A\n",
      "  0%|          | 501/198607 [00:27<3:53:24, 14.15it/s]\u001b[A\n",
      "  0%|          | 503/198607 [00:27<3:59:55, 13.76it/s]\u001b[A\n",
      "  0%|          | 505/198607 [00:27<3:57:45, 13.89it/s]\u001b[A\n",
      "  0%|          | 507/198607 [00:27<3:54:14, 14.10it/s]\u001b[A\n",
      "  0%|          | 509/198607 [00:27<3:54:53, 14.06it/s]\u001b[A\n",
      "  0%|          | 511/198607 [00:28<3:56:34, 13.96it/s]\u001b[A\n",
      "  0%|          | 513/198607 [00:28<3:51:40, 14.25it/s]\u001b[A\n",
      "  0%|          | 515/198607 [00:28<3:51:42, 14.25it/s]\u001b[A\n",
      "  0%|          | 517/198607 [00:28<3:51:17, 14.27it/s]\u001b[A\n",
      "  0%|          | 519/198607 [00:28<3:56:49, 13.94it/s]\u001b[A\n",
      "  0%|          | 521/198607 [00:28<3:52:35, 14.19it/s]\u001b[A\n",
      "  0%|          | 523/198607 [00:28<3:49:48, 14.37it/s]\u001b[A\n",
      "  0%|          | 525/198607 [00:29<3:51:19, 14.27it/s]\u001b[A\n",
      "  0%|          | 527/198607 [00:29<3:51:38, 14.25it/s]\u001b[A\n",
      "  0%|          | 529/198607 [00:29<4:00:51, 13.71it/s]\u001b[A\n",
      "  0%|          | 531/198607 [00:29<3:58:21, 13.85it/s]\u001b[A\n",
      "  0%|          | 533/198607 [00:29<3:56:03, 13.99it/s]\u001b[A\n",
      "  0%|          | 535/198607 [00:29<4:00:21, 13.73it/s]\u001b[A\n",
      "  0%|          | 537/198607 [00:29<3:57:21, 13.91it/s]\u001b[A\n",
      "  0%|          | 539/198607 [00:30<3:55:46, 14.00it/s]\u001b[A\n",
      "  0%|          | 541/198607 [00:30<3:54:06, 14.10it/s]\u001b[A\n",
      "  0%|          | 543/198607 [00:30<3:51:55, 14.23it/s]\u001b[A\n",
      "  0%|          | 545/198607 [00:30<3:51:26, 14.26it/s]\u001b[A\n",
      "  0%|          | 547/198607 [00:30<3:52:01, 14.23it/s]\u001b[A\n",
      "  0%|          | 549/198607 [00:30<3:51:17, 14.27it/s]\u001b[A\n",
      "  0%|          | 551/198607 [00:30<3:56:07, 13.98it/s]\u001b[A\n",
      "  0%|          | 553/198607 [00:31<3:55:40, 14.01it/s]\u001b[A\n",
      "  0%|          | 555/198607 [00:31<3:51:11, 14.28it/s]\u001b[A\n",
      "  0%|          | 557/198607 [00:31<3:51:48, 14.24it/s]\u001b[A\n",
      "  0%|          | 559/198607 [00:31<3:50:17, 14.33it/s]\u001b[A\n",
      "  0%|          | 561/198607 [00:31<3:52:40, 14.19it/s]\u001b[A\n",
      "  0%|          | 563/198607 [00:31<3:52:34, 14.19it/s]\u001b[A\n",
      "  0%|          | 565/198607 [00:31<3:50:33, 14.32it/s]\u001b[A\n",
      "  0%|          | 567/198607 [00:32<3:54:21, 14.08it/s]\u001b[A\n",
      "  0%|          | 569/198607 [00:32<3:49:01, 14.41it/s]\u001b[A\n",
      "  0%|          | 571/198607 [00:32<3:50:34, 14.31it/s]\u001b[A\n",
      "  0%|          | 573/198607 [00:32<3:50:17, 14.33it/s]\u001b[A\n",
      "  0%|          | 575/198607 [00:32<3:51:54, 14.23it/s]\u001b[A\n",
      "  0%|          | 577/198607 [00:32<3:51:27, 14.26it/s]\u001b[A\n",
      "  0%|          | 579/198607 [00:32<3:49:43, 14.37it/s]\u001b[A\n",
      "  0%|          | 581/198607 [00:33<3:50:35, 14.31it/s]\u001b[A\n",
      "  0%|          | 583/198607 [00:33<3:57:30, 13.90it/s]\u001b[A\n",
      "  0%|          | 585/198607 [00:33<3:52:24, 14.20it/s]\u001b[A\n",
      "  0%|          | 587/198607 [00:33<3:53:14, 14.15it/s]\u001b[A\n",
      "  0%|          | 589/198607 [00:33<3:51:46, 14.24it/s]\u001b[A\n",
      "  0%|          | 591/198607 [00:33<3:51:17, 14.27it/s]\u001b[A\n",
      "  0%|          | 593/198607 [00:33<3:50:03, 14.35it/s]\u001b[A\n",
      "  0%|          | 595/198607 [00:34<3:48:40, 14.43it/s]\u001b[A\n",
      "  0%|          | 597/198607 [00:34<3:50:20, 14.33it/s]\u001b[A\n",
      "  0%|          | 599/198607 [00:34<3:56:33, 13.95it/s]\u001b[A\n",
      "  0%|          | 601/198607 [00:34<3:51:35, 14.25it/s]\u001b[A\n",
      "  0%|          | 603/198607 [00:34<4:13:23, 13.02it/s]\u001b[A\n",
      "  0%|          | 605/198607 [00:34<4:07:24, 13.34it/s]\u001b[A\n",
      "  0%|          | 607/198607 [00:34<4:03:10, 13.57it/s]\u001b[A\n",
      "  0%|          | 609/198607 [00:35<3:59:42, 13.77it/s]\u001b[A\n",
      "  0%|          | 611/198607 [00:35<3:57:16, 13.91it/s]\u001b[A\n",
      "  0%|          | 615/198607 [00:35<3:25:19, 16.07it/s]\u001b[A\n",
      "  0%|          | 619/198607 [00:35<3:01:38, 18.17it/s]\u001b[A\n",
      "  0%|          | 623/198607 [00:35<2:46:09, 19.86it/s]\u001b[A\n",
      "  0%|          | 626/198607 [00:35<3:05:21, 17.80it/s]\u001b[A\n",
      "  0%|          | 629/198607 [00:36<2:56:55, 18.65it/s]\u001b[A\n",
      "  0%|          | 632/198607 [00:36<2:54:11, 18.94it/s]\u001b[A\n",
      "  0%|          | 636/198607 [00:36<2:40:57, 20.50it/s]\u001b[A\n",
      "  0%|          | 639/198607 [00:36<2:46:32, 19.81it/s]\u001b[A\n",
      "  0%|          | 642/198607 [00:36<2:47:17, 19.72it/s]\u001b[A\n",
      "  0%|          | 645/198607 [00:36<3:06:14, 17.72it/s]\u001b[A\n",
      "  0%|          | 647/198607 [00:36<3:16:28, 16.79it/s]\u001b[A\n",
      "  0%|          | 652/198607 [00:37<2:37:54, 20.89it/s]\u001b[A\n",
      "  0%|          | 655/198607 [00:37<2:38:45, 20.78it/s]\u001b[A\n",
      "  0%|          | 658/198607 [00:37<2:40:23, 20.57it/s]\u001b[A\n",
      "  0%|          | 661/198607 [00:37<2:39:56, 20.63it/s]\u001b[A\n",
      "  0%|          | 664/198607 [00:37<2:40:42, 20.53it/s]\u001b[A\n",
      "  0%|          | 667/198607 [00:37<3:03:06, 18.02it/s]\u001b[A\n",
      "  0%|          | 678/198607 [00:37<2:17:25, 24.01it/s]\u001b[A\n",
      "  0%|          | 694/198607 [00:38<1:42:33, 32.16it/s]\u001b[A\n",
      "  0%|          | 702/198607 [00:38<1:26:10, 38.28it/s]\u001b[A\n",
      "  0%|          | 710/198607 [00:38<1:14:36, 44.21it/s]\u001b[A\n",
      "  0%|          | 718/198607 [00:38<1:06:10, 49.84it/s]\u001b[A\n",
      "  0%|          | 726/198607 [00:38<1:02:35, 52.69it/s]\u001b[A\n",
      "  0%|          | 733/198607 [00:38<58:41, 56.19it/s]  \u001b[A\n",
      "  0%|          | 740/198607 [00:38<56:36, 58.26it/s]\u001b[A\n",
      "  0%|          | 748/198607 [00:38<52:15, 63.10it/s]\u001b[A\n",
      "  0%|          | 756/198607 [00:39<51:35, 63.91it/s]\u001b[A\n",
      "  0%|          | 772/198607 [00:39<42:40, 77.26it/s]\u001b[A\n",
      "  0%|          | 784/198607 [00:39<38:32, 85.53it/s]\u001b[A\n",
      "  0%|          | 795/198607 [00:39<37:23, 88.19it/s]\u001b[A\n",
      "  0%|          | 806/198607 [00:39<37:21, 88.26it/s]\u001b[A\n",
      "  0%|          | 816/198607 [00:39<40:37, 81.16it/s]\u001b[A\n",
      "  0%|          | 825/198607 [00:39<42:54, 76.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 834/198607 [00:39<42:33, 77.46it/s]\u001b[A\n",
      "  0%|          | 843/198607 [00:39<42:09, 78.19it/s]\u001b[A\n",
      "  0%|          | 858/198607 [00:40<36:06, 91.27it/s]\u001b[A\n",
      "  0%|          | 889/198607 [00:40<28:41, 114.87it/s]\u001b[A\n",
      "  0%|          | 913/198607 [00:40<24:14, 135.92it/s]\u001b[A\n",
      "  0%|          | 943/198607 [00:40<20:38, 159.63it/s]\u001b[A\n",
      "  0%|          | 973/198607 [00:40<17:45, 185.47it/s]\u001b[A\n",
      "  1%|          | 997/198607 [00:40<18:40, 176.32it/s]\u001b[A\n",
      "  1%|          | 1023/198607 [00:40<16:54, 194.76it/s]\u001b[A\n",
      "  1%|          | 1054/198607 [00:40<15:04, 218.31it/s]\u001b[A\n",
      "  1%|          | 1089/198607 [00:40<13:24, 245.48it/s]\u001b[A\n",
      "  1%|          | 1132/198607 [00:41<11:43, 280.53it/s]\u001b[A\n",
      "  1%|          | 1166/198607 [00:41<11:10, 294.49it/s]\u001b[A\n",
      "  1%|          | 1199/198607 [00:41<11:06, 296.28it/s]\u001b[A\n",
      "  1%|          | 1231/198607 [00:45<2:25:20, 22.63it/s]\u001b[A\n",
      "  1%|          | 1254/198607 [00:50<5:09:26, 10.63it/s]\u001b[A\n",
      "  1%|          | 1270/198607 [00:54<7:11:58,  7.61it/s]\u001b[A\n",
      "  1%|          | 1282/198607 [00:56<8:16:26,  6.62it/s]\u001b[A\n",
      "  1%|          | 1291/198607 [00:58<9:16:35,  5.91it/s]\u001b[A\n",
      "  1%|          | 1297/198607 [00:59<10:00:33,  5.48it/s]\u001b[A\n",
      "  1%|          | 1302/198607 [01:00<10:30:49,  5.21it/s]\u001b[A\n",
      "  1%|          | 1306/198607 [01:01<10:53:10,  5.03it/s]\u001b[A\n",
      "  1%|          | 1309/198607 [01:02<11:08:20,  4.92it/s]\u001b[A\n",
      "  1%|          | 1311/198607 [01:02<11:22:54,  4.82it/s]\u001b[A\n",
      "  1%|          | 1313/198607 [01:03<11:34:52,  4.73it/s]\u001b[A\n",
      "  1%|          | 1315/198607 [01:03<11:35:31,  4.73it/s]\u001b[A\n",
      "  1%|          | 1316/198607 [01:03<11:44:14,  4.67it/s]\u001b[A\n",
      "  1%|          | 1317/198607 [01:03<11:51:05,  4.62it/s]\u001b[A\n",
      "  1%|          | 1318/198607 [01:04<11:44:00,  4.67it/s]\u001b[A\n",
      "  1%|          | 1319/198607 [01:04<11:50:11,  4.63it/s]\u001b[A\n",
      "  1%|          | 1320/198607 [01:04<11:46:25,  4.65it/s]\u001b[A\n",
      "  1%|          | 1321/198607 [01:04<11:52:11,  4.62it/s]\u001b[A\n",
      "  1%|          | 1322/198607 [01:04<11:51:39,  4.62it/s]\u001b[A\n",
      "  1%|          | 1323/198607 [01:05<11:50:53,  4.63it/s]\u001b[A\n",
      "  1%|          | 1324/198607 [01:05<11:51:29,  4.62it/s]\u001b[A\n",
      "  1%|          | 1325/198607 [01:05<11:59:35,  4.57it/s]\u001b[A\n",
      "  1%|          | 1326/198607 [01:05<12:00:17,  4.56it/s]\u001b[A\n",
      "  1%|          | 1327/198607 [01:06<12:05:22,  4.53it/s]\u001b[A\n",
      "  1%|          | 1328/198607 [01:06<12:03:17,  4.55it/s]\u001b[A\n",
      "  1%|          | 1329/198607 [01:06<11:57:34,  4.58it/s]\u001b[A\n",
      "  1%|          | 1330/198607 [01:06<11:55:39,  4.59it/s]\u001b[A\n",
      "  1%|          | 1331/198607 [01:06<11:53:40,  4.61it/s]\u001b[A\n",
      "  1%|          | 1332/198607 [01:07<11:49:20,  4.64it/s]\u001b[A\n",
      "  1%|          | 1333/198607 [01:07<11:47:33,  4.65it/s]\u001b[A\n",
      "  1%|          | 1334/198607 [01:07<11:45:17,  4.66it/s]\u001b[A\n",
      "  1%|          | 1335/198607 [01:07<12:15:18,  4.47it/s]\u001b[A\n",
      "  1%|          | 1336/198607 [01:08<12:10:32,  4.50it/s]\u001b[A\n",
      "  1%|          | 1337/198607 [01:08<12:28:27,  4.39it/s]\u001b[A\n",
      "  1%|          | 1338/198607 [01:08<12:17:08,  4.46it/s]\u001b[A\n",
      "  1%|          | 1339/198607 [01:08<12:11:18,  4.50it/s]\u001b[A\n",
      "  1%|          | 1340/198607 [01:08<12:02:14,  4.55it/s]\u001b[A\n",
      "  1%|          | 1341/198607 [01:09<11:45:35,  4.66it/s]\u001b[A\n",
      "  1%|          | 1342/198607 [01:09<11:53:18,  4.61it/s]\u001b[A\n",
      "  1%|          | 1343/198607 [01:09<11:56:52,  4.59it/s]\u001b[A\n",
      "  1%|          | 1344/198607 [01:09<11:44:03,  4.67it/s]\u001b[A\n",
      "  1%|          | 1345/198607 [01:10<11:46:45,  4.65it/s]\u001b[A\n",
      "  1%|          | 1346/198607 [01:10<11:47:46,  4.65it/s]\u001b[A\n",
      "  1%|          | 1347/198607 [01:10<11:50:48,  4.63it/s]\u001b[A\n",
      "  1%|          | 1348/198607 [01:10<11:47:45,  4.65it/s]\u001b[A\n",
      "  1%|          | 1349/198607 [01:10<12:02:07,  4.55it/s]\u001b[A\n",
      "  1%|          | 1350/198607 [01:11<11:56:45,  4.59it/s]\u001b[A\n",
      "  1%|          | 1351/198607 [01:11<11:53:55,  4.60it/s]\u001b[A\n",
      "  1%|          | 1352/198607 [01:11<11:56:03,  4.59it/s]\u001b[A\n",
      "  1%|          | 1353/198607 [01:11<11:55:10,  4.60it/s]\u001b[A\n",
      "  1%|          | 1354/198607 [01:11<12:04:22,  4.54it/s]\u001b[A\n",
      "  1%|          | 1355/198607 [01:12<11:59:04,  4.57it/s]\u001b[A\n",
      "  1%|          | 1356/198607 [01:12<11:54:21,  4.60it/s]\u001b[A\n",
      "  1%|          | 1357/198607 [01:12<11:56:54,  4.59it/s]\u001b[A\n",
      "  1%|          | 1358/198607 [01:12<11:50:12,  4.63it/s]\u001b[A\n",
      "  1%|          | 1359/198607 [01:13<11:49:49,  4.63it/s]\u001b[A\n",
      "  1%|          | 1360/198607 [01:13<11:54:59,  4.60it/s]\u001b[A\n",
      "  1%|          | 1361/198607 [01:13<11:44:14,  4.67it/s]\u001b[A\n",
      "  1%|          | 1362/198607 [01:13<11:50:23,  4.63it/s]\u001b[A\n",
      "  1%|          | 1363/198607 [01:13<11:49:32,  4.63it/s]\u001b[A\n",
      "  1%|          | 1364/198607 [01:14<11:50:54,  4.62it/s]\u001b[A\n",
      "  1%|          | 1365/198607 [01:14<11:47:33,  4.65it/s]\u001b[A\n",
      "  1%|          | 1366/198607 [01:14<11:33:57,  4.74it/s]\u001b[A\n",
      "  1%|          | 1367/198607 [01:14<11:44:55,  4.66it/s]\u001b[A\n",
      "  1%|          | 1368/198607 [01:14<11:51:49,  4.62it/s]\u001b[A\n",
      "  1%|          | 1369/198607 [01:15<11:53:31,  4.61it/s]\u001b[A\n",
      "  1%|          | 1370/198607 [01:15<11:55:32,  4.59it/s]\u001b[A\n",
      "  1%|          | 1371/198607 [01:15<11:55:14,  4.60it/s]\u001b[A\n",
      "  1%|          | 1372/198607 [01:15<11:48:56,  4.64it/s]\u001b[A\n",
      "  1%|          | 1373/198607 [01:16<11:53:23,  4.61it/s]\u001b[A\n",
      "  1%|          | 1374/198607 [01:16<11:53:32,  4.61it/s]\u001b[A\n",
      "  1%|          | 1375/198607 [01:16<11:52:14,  4.62it/s]\u001b[A\n",
      "  1%|          | 1376/198607 [01:16<11:55:02,  4.60it/s]\u001b[A\n",
      "  1%|          | 1377/198607 [01:16<12:03:29,  4.54it/s]\u001b[A\n",
      "  1%|          | 1378/198607 [01:17<11:54:18,  4.60it/s]\u001b[A\n",
      "  1%|          | 1379/198607 [01:17<11:58:21,  4.58it/s]\u001b[A\n",
      "  1%|          | 1380/198607 [01:17<11:53:23,  4.61it/s]\u001b[A\n",
      "  1%|          | 1381/198607 [01:17<11:40:37,  4.69it/s]\u001b[A\n",
      "  1%|          | 1382/198607 [01:18<11:46:29,  4.65it/s]\u001b[A\n",
      "  1%|          | 1383/198607 [01:18<11:44:44,  4.66it/s]\u001b[A\n",
      "  1%|          | 1384/198607 [01:18<11:46:36,  4.65it/s]\u001b[A\n",
      "  1%|          | 1385/198607 [01:18<11:53:59,  4.60it/s]\u001b[A\n",
      "  1%|          | 1386/198607 [01:18<11:58:36,  4.57it/s]\u001b[A\n",
      "  1%|          | 1387/198607 [01:19<12:02:19,  4.55it/s]\u001b[A\n",
      "  1%|          | 1388/198607 [01:19<11:58:47,  4.57it/s]\u001b[A\n",
      "  1%|          | 1389/198607 [01:19<11:45:40,  4.66it/s]\u001b[A\n",
      "  1%|          | 1390/198607 [01:19<11:35:42,  4.72it/s]\u001b[A\n",
      "  1%|          | 1391/198607 [01:19<11:28:53,  4.77it/s]\u001b[A\n",
      "  1%|          | 1392/198607 [01:20<11:39:23,  4.70it/s]\u001b[A\n",
      "  1%|          | 1393/198607 [01:20<11:33:48,  4.74it/s]\u001b[A\n",
      "  1%|          | 1394/198607 [01:20<11:40:23,  4.69it/s]\u001b[A\n",
      "  1%|          | 1395/198607 [01:20<11:41:43,  4.68it/s]\u001b[A\n",
      "  1%|          | 1396/198607 [01:21<11:52:57,  4.61it/s]\u001b[A\n",
      "  1%|          | 1397/198607 [01:21<11:54:05,  4.60it/s]\u001b[A\n",
      "  1%|          | 1398/198607 [01:21<11:56:11,  4.59it/s]\u001b[A\n",
      "  1%|          | 1399/198607 [01:21<11:59:11,  4.57it/s]\u001b[A\n",
      "  1%|          | 1400/198607 [01:21<11:59:45,  4.57it/s]\u001b[A\n",
      "  1%|          | 1401/198607 [01:22<12:02:48,  4.55it/s]\u001b[A\n",
      "  1%|          | 1402/198607 [01:22<12:09:41,  4.50it/s]\u001b[A\n",
      "  1%|          | 1403/198607 [01:22<12:09:23,  4.51it/s]\u001b[A\n",
      "  1%|          | 1404/198607 [01:22<12:26:12,  4.40it/s]\u001b[A\n",
      "  1%|          | 1405/198607 [01:23<12:07:31,  4.52it/s]\u001b[A\n",
      "  1%|          | 1406/198607 [01:23<12:02:25,  4.55it/s]\u001b[A\n",
      "  1%|          | 1407/198607 [01:23<12:03:23,  4.54it/s]\u001b[A\n",
      "  1%|          | 1408/198607 [01:23<11:58:53,  4.57it/s]\u001b[A\n",
      "  1%|          | 1409/198607 [01:23<11:56:59,  4.58it/s]\u001b[A\n",
      "  1%|          | 1410/198607 [01:24<11:52:45,  4.61it/s]\u001b[A\n",
      "  1%|          | 1411/198607 [01:24<11:53:31,  4.61it/s]\u001b[A\n",
      "  1%|          | 1412/198607 [01:24<11:59:02,  4.57it/s]\u001b[A\n",
      "  1%|          | 1413/198607 [01:24<11:57:07,  4.58it/s]\u001b[A\n",
      "  1%|          | 1414/198607 [01:24<11:57:20,  4.58it/s]\u001b[A\n",
      "  1%|          | 1416/198607 [01:25<10:31:54,  5.20it/s]\u001b[A\n",
      "  1%|          | 1417/198607 [01:25<10:43:48,  5.10it/s]\u001b[A\n",
      "  1%|          | 1418/198607 [01:25<10:57:16,  5.00it/s]\u001b[A\n",
      "  1%|          | 1419/198607 [01:25<11:36:23,  4.72it/s]\u001b[A\n",
      "  1%|          | 1420/198607 [01:26<11:42:58,  4.68it/s]\u001b[A\n",
      "  1%|          | 1421/198607 [01:26<11:41:31,  4.68it/s]\u001b[A\n",
      "  1%|          | 1422/198607 [01:26<11:30:57,  4.76it/s]\u001b[A\n",
      "  1%|          | 1423/198607 [01:26<11:40:03,  4.69it/s]\u001b[A\n",
      "  1%|          | 1424/198607 [01:26<11:28:14,  4.78it/s]\u001b[A\n",
      "  1%|          | 1425/198607 [01:27<11:31:11,  4.75it/s]\u001b[A\n",
      "  1%|          | 1426/198607 [01:27<11:33:01,  4.74it/s]\u001b[A\n",
      "  1%|          | 1427/198607 [01:27<11:33:52,  4.74it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1428/198607 [01:27<11:37:54,  4.71it/s]\u001b[A\n",
      "  1%|          | 1429/198607 [01:28<11:30:09,  4.76it/s]\u001b[A\n",
      "  1%|          | 1430/198607 [01:28<11:34:31,  4.73it/s]\u001b[A\n",
      "  1%|          | 1431/198607 [01:28<11:36:37,  4.72it/s]\u001b[A\n",
      "  1%|          | 1432/198607 [01:28<11:25:54,  4.79it/s]\u001b[A\n",
      "  1%|          | 1433/198607 [01:28<11:35:09,  4.73it/s]\u001b[A\n",
      "  1%|          | 1434/198607 [01:29<11:28:20,  4.77it/s]\u001b[A\n",
      "  1%|          | 1435/198607 [01:29<11:24:46,  4.80it/s]\u001b[A\n",
      "  1%|          | 1436/198607 [01:29<11:30:52,  4.76it/s]\u001b[A\n",
      "  1%|          | 1437/198607 [01:29<11:30:42,  4.76it/s]\u001b[A\n",
      "  1%|          | 1438/198607 [01:29<11:35:57,  4.72it/s]\u001b[A\n",
      "  1%|          | 1439/198607 [01:30<11:41:41,  4.68it/s]\u001b[A\n",
      "  1%|          | 1440/198607 [01:30<11:40:59,  4.69it/s]\u001b[A\n",
      "  1%|          | 1441/198607 [01:30<11:40:41,  4.69it/s]\u001b[A\n",
      "  1%|          | 1442/198607 [01:30<11:29:04,  4.77it/s]\u001b[A\n",
      "  1%|          | 1443/198607 [01:30<11:26:02,  4.79it/s]\u001b[A\n",
      "  1%|          | 1444/198607 [01:31<11:33:36,  4.74it/s]\u001b[A\n",
      "  1%|          | 1445/198607 [01:31<11:38:42,  4.70it/s]\u001b[A\n",
      "  1%|          | 1446/198607 [01:31<11:37:45,  4.71it/s]\u001b[A\n",
      "  1%|          | 1447/198607 [01:31<11:39:05,  4.70it/s]\u001b[A\n",
      "  1%|          | 1448/198607 [01:32<11:46:29,  4.65it/s]\u001b[A\n",
      "  1%|          | 1449/198607 [01:32<11:46:01,  4.65it/s]\u001b[A\n",
      "  1%|          | 1450/198607 [01:32<11:45:04,  4.66it/s]\u001b[A\n",
      "  1%|          | 1451/198607 [01:32<11:47:07,  4.65it/s]\u001b[A\n",
      "  1%|          | 1452/198607 [01:32<11:47:49,  4.64it/s]\u001b[A\n",
      "  1%|          | 1453/198607 [01:33<11:48:08,  4.64it/s]\u001b[A\n",
      "  1%|          | 1454/198607 [01:33<11:44:40,  4.66it/s]\u001b[A\n",
      "  1%|          | 1455/198607 [01:33<10:45:51,  5.09it/s]\u001b[A\n",
      "  1%|          | 1456/198607 [01:33<11:04:39,  4.94it/s]\u001b[A\n",
      "  1%|          | 1457/198607 [01:33<11:22:03,  4.82it/s]\u001b[A\n",
      "  1%|          | 1458/198607 [01:34<11:37:56,  4.71it/s]\u001b[A\n",
      "  1%|          | 1459/198607 [01:34<11:37:35,  4.71it/s]\u001b[A\n",
      "  1%|          | 1460/198607 [01:34<11:39:01,  4.70it/s]\u001b[A\n",
      "  1%|          | 1461/198607 [01:34<11:40:12,  4.69it/s]\u001b[A\n",
      "  1%|          | 1462/198607 [01:35<11:41:49,  4.68it/s]\u001b[A\n",
      "  1%|          | 1463/198607 [01:35<11:45:56,  4.65it/s]\u001b[A\n",
      "  1%|          | 1464/198607 [01:35<11:53:23,  4.61it/s]\u001b[A\n",
      "  1%|          | 1465/198607 [01:35<11:59:00,  4.57it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a5d5c7aa65cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#             neg_features_author.append(process_feature_coauthor(neg_ins, paper_coauthors))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         pos_features_author.extend([process_feature_coauthor(xx, paper_coauthors)\n\u001b[0;32m---> 57\u001b[0;31m                                     for xx in pos_set])\n\u001b[0m\u001b[1;32m     58\u001b[0m         neg_features_author.extend([process_feature_coauthor(xx, paper_coauthors)\n\u001b[1;32m     59\u001b[0m                                     for xx in neg_set])\n",
      "\u001b[0;32m<ipython-input-6-a5d5c7aa65cb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#             neg_features_author.append(process_feature_coauthor(neg_ins, paper_coauthors))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         pos_features_author.extend([process_feature_coauthor(xx, paper_coauthors)\n\u001b[0;32m---> 57\u001b[0;31m                                     for xx in pos_set])\n\u001b[0m\u001b[1;32m     58\u001b[0m         neg_features_author.extend([process_feature_coauthor(xx, paper_coauthors)\n\u001b[1;32m     59\u001b[0m                                     for xx in neg_set])\n",
      "\u001b[0;32m<ipython-input-3-a63b8725324b>\u001b[0m in \u001b[0;36mprocess_feature_coauthor\u001b[0;34m(pos_ins, paper_coauthors)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# 获取paper中main author_name所对应的位置\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelete_main_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaper_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# 获取除了main author_name外的coauthor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a63b8725324b>\u001b[0m in \u001b[0;36mdelete_main_name\u001b[0;34m(author_list, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthor_list_lower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# lower_name = author.lower()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_jaro_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinkler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mauthor_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_split\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pyjarowinkler/distance.py\u001b[0m in \u001b[0;36mget_jaro_distance\u001b[0;34m(first, second, winkler, winkler_ajustment, scaling)\u001b[0m\n\u001b[1;32m     30\u001b[0m             second.__class__.__name__))\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mjaro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pyjarowinkler/distance.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(first, second)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mlonger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshorter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshorter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlonger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_matching_characters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlonger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_matching_characters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlonger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshorter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pyjarowinkler/distance.py\u001b[0m in \u001b[0;36m_get_matching_characters\u001b[0;34m(first, second)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0msecond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msecond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msecond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "pos_features_author = []\n",
    "neg_features_author = []\n",
    "pos_features_keywords = []\n",
    "neg_features_keywords = []\n",
    "pos_features_org = []\n",
    "neg_features_org = []\n",
    "print(len(train_instances))\n",
    "\n",
    "for ins in tqdm.tqdm(train_instances):\n",
    "    \n",
    "    pos_set = ins[0]\n",
    "    neg_set = ins[1]\n",
    "    paper_id = list(pos_set)[0][0]\n",
    "    paper_name = paper2aid2name[paper_id][0]\n",
    "    author_name=None\n",
    "    \n",
    "    author_list = []\n",
    "    # 获取paper的coauthors\n",
    "    paper_coauthors = []\n",
    "    feature_coauthor=[]\n",
    "    \n",
    "    paper_authors = pubs_dict[paper_id]['authors']\n",
    "    paper_authors_len = len(paper_authors)\n",
    "    # 只取前50个author以保证效率\n",
    "#     paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "\n",
    "#     for author in paper_authors:                \n",
    "#         clean_author = clean_name(author['name'])\n",
    "#         if(clean_author != None):\n",
    "#             author_list.append(clean_author)\n",
    "    author_list.extend([x \n",
    "                        for x in [clean_name(xx['name']) \n",
    "                                  for xx in paper_authors] \n",
    "                        if x is not None])  # simplify code\n",
    "    \n",
    "    if(len(author_list) > 0):\n",
    "        # 获取paper中main author_name所对应的位置\n",
    "        _, author_index = delete_main_name(author_list, paper_name)\n",
    "        \n",
    "        # 获取除了main author_name外的coauthor\n",
    "        for index in range(len(author_list)):\n",
    "            if(index == author_index):\n",
    "                author_name=author_list[author_index]\n",
    "                continue\n",
    "            else:\n",
    "                paper_coauthors.append(author_list[index])\n",
    "        \n",
    "    \n",
    "#         for pos_ins in pos_set:\n",
    "#             pos_features_author.append(process_feature_coauthor(pos_ins, paper_coauthors))\n",
    "#         for neg_ins in neg_set:\n",
    "#             neg_features_author.append(process_feature_coauthor(neg_ins, paper_coauthors))\n",
    "        pos_features_author.extend([process_feature_coauthor(xx, paper_coauthors)\n",
    "                                    for xx in pos_set])\n",
    "        neg_features_author.extend([process_feature_coauthor(xx, paper_coauthors)\n",
    "                                    for xx in neg_set])\n",
    "    else:\n",
    "#         for pos_ins in pos_set:\n",
    "#             pos_features_author.append([0.] * 5)\n",
    "#         for neg_ins in neg_set:\n",
    "#             neg_features_author.append([0.] * 5)\n",
    "        pos_features_author.extend([[0.] * 5] * len(pos_set))\n",
    "        neg_features_author.extend([[0.] * 5] * len(neg_set))\n",
    "        \n",
    "#         处理话题\n",
    "    topic_list=[]\n",
    "    if 'keywords' in pubs_dict[paper_id].keys():\n",
    "        paper_topics=pubs_dict[paper_id]['keywords']\n",
    "    \n",
    "    paper_topics_len=len(paper_topics)\n",
    "    paper_topics=random.sample(paper_topics,min(10,paper_topics_len))\n",
    "    \n",
    "#     for topic in paper_topics:\n",
    "#         clean_topic=clean_name(topic)\n",
    "#         if clean_topic!=None:\n",
    "#             topic_list.append(clean_topic)\n",
    "    topic_list.extend([x\n",
    "                       for x in [clean_name(xx)\n",
    "                                 for xx in paper_topics]\n",
    "                       if x is not None])\n",
    "    if len(topic_list)>0:\n",
    "#         for pos_ins in pos_set:\n",
    "#             pos_features_keywords.append(process_feature_keywords(pos_ins,topic_list))\n",
    "#         for neg_ins in neg_set:\n",
    "#             neg_features_keywords.append(process_feature_keywords(neg_ins,topic_list))\n",
    "        pos_features_keywords.extend([process_feature_keywords(xx, topic_list)\n",
    "                                      for xx in pos_set])\n",
    "        neg_features_keywords.extend([process_feature_keywords(xx, topic_list)\n",
    "                                      for xx in neg_set])\n",
    "    else:\n",
    "#         for pos_ins in pos_set:\n",
    "#             pos_features_keywords.append([0.] * 5)\n",
    "#         for neg_ins in neg_set:\n",
    "#             neg_features_keywords.append([0.] * 5)\n",
    "        pos_features_keywords.extend([[0.] * 5] * len(pos_set))\n",
    "        neg_features_keywords.extend([[0.] * 5] * len(neg_set))\n",
    "            \n",
    "            \n",
    "#     #         处理org\n",
    "    \n",
    "    \n",
    "#     name = clean_name(paper_name)\n",
    "#     author_list_lower = []\n",
    "#     for author in author_list:\n",
    "#         author_list_lower.append(author.lower())\n",
    "#     name_split = name.split()\n",
    "#     org=None\n",
    "    \n",
    "# #     print(pubs_dict[paper_id].keys())\n",
    "#     paper_authors=pubs_dict[paper_id]['authors']\n",
    "#     for author in paper_authors:\n",
    "#         aname=clean_name(author['name'])\n",
    "#         if aname==author_name:\n",
    "# #             print('in')\n",
    "#             if 'org' in author.keys():\n",
    "#                 org=author['org']\n",
    "#             break\n",
    "    \n",
    "#     if org!=None and org!='':\n",
    "# #         print(1,org)\n",
    "#         for pos_ins in pos_set:\n",
    "#             pos_features_org.append(process_feature_org(pos_ins, org, author_name))\n",
    "#         for neg_ins in neg_set:\n",
    "#             neg_features_org.append(process_feature_org(neg_ins, org, author_name))\n",
    "#     else:\n",
    "#         for pos_ins in pos_set:\n",
    "#             pos_features_org.append([0.])\n",
    "#         for neg_ins in neg_set:\n",
    "#             neg_features_org.append([0.] )\n",
    "\n",
    "# pos_features=[]\n",
    "# neg_features=[]\n",
    "# for i in range(len(pos_features_author)):\n",
    "#     pos_features.append(pos_features_author[i]+pos_features_keywords[i])\n",
    "# for i in range(len(neg_features_author)):\n",
    "#     neg_features.append(neg_features_author[i]+neg_features_keywords[i])\n",
    "pos_features = [pos_features_author[i] + pos_features_keywords[i]\n",
    "                     for i in range(len(pos_features_author))]\n",
    "neg_features = [neg_features_author[i] + neg_features_keywords[i]\n",
    "                     for i in range(len(neg_features_author))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"pos_features.txt\",pos_features,fmt='%f',delimiter=',' )\n",
    "# np.savetxt(\"neg_features.txt\",neg_features,fmt='%f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_features= np.loadtxt('pos_features.txt',delimiter=',')\n",
    "# neg_features= np.loadtxt('neg_features.txt',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.externals import joblib\n",
    "# from sklearn import tree\n",
    "# # 构建svm正负例\n",
    "# svm_train_ins = []\n",
    "# for ins in pos_features:\n",
    "#     svm_train_ins.append((ins, 1))\n",
    "\n",
    "# for ins in neg_features:\n",
    "#     svm_train_ins.append((ins, 0))\n",
    "\n",
    "# print(np.array(svm_train_ins).shape)\n",
    "\n",
    "# random.shuffle(svm_train_ins)\n",
    "\n",
    "# x_train= []\n",
    "# y_train = []\n",
    "# for ins in svm_train_ins:\n",
    "#     x_train.append(ins[0])\n",
    "#     y_train.append(ins[1])\n",
    "\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191642, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zengrui/miniconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.externals import joblib\n",
    "\n",
    "# # 构建svm正负例\n",
    "# svm_train_ins = []\n",
    "# for ins in pos_features:\n",
    "#     svm_train_ins.append((ins, 1))\n",
    "\n",
    "# for ins in neg_features:\n",
    "#     svm_train_ins.append((ins, 0))\n",
    "\n",
    "# print(np.array(svm_train_ins).shape)\n",
    "\n",
    "# random.shuffle(svm_train_ins)\n",
    "\n",
    "# x_train= []\n",
    "# y_train = []\n",
    "# for ins in svm_train_ins:\n",
    "#     x_train.append(ins[0])\n",
    "#     y_train.append(ins[1])\n",
    "\n",
    "# clf = SVC(probability=True)\n",
    "# clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ins = []\n",
    "# for ins in pos_features:\n",
    "#     train_ins.append((ins, 1))\n",
    "\n",
    "# for ins in neg_features:\n",
    "#     train_ins.append((ins, 0))\n",
    "\n",
    "# print(np.array(train_ins).shape)\n",
    "\n",
    "# random.shuffle(train_ins)\n",
    "\n",
    "# x_train= []\n",
    "# y_train = []\n",
    "# for ins in train_ins:\n",
    "#     x_train.append(ins[0])\n",
    "#     y_train.append(ins[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder=OneHotEncoder(sparse=False)\n",
    "# yy=[[0],[1]]\n",
    "# encoder.fit(yy)\n",
    "# y_train_reshape=np.array(y_train).reshape(-1,1)\n",
    "# y_train_onehot=encoder.transform(y_train_reshape)\n",
    "# print(y_train_onehot[1:10])\n",
    "\n",
    "# # y_test_reshape=np.array(y_test).reshape(-1,1)\n",
    "# # y_test_onehot=encoder.transform(y_test_reshape)\n",
    "# # print(y_test_onehot[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train=np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1191642, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zengrui/miniconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Train on 1179725 samples, validate on 11917 samples\n",
      "Epoch 1/100\n",
      "1179725/1179725 [==============================] - 11s 9us/sample - loss: 0.1401 - accuracy: 0.9670 - val_loss: 0.7278 - val_accuracy: 0.2110\n",
      "Epoch 2/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1191 - accuracy: 0.9720 - val_loss: 0.2944 - val_accuracy: 0.9666\n",
      "Epoch 3/100\n",
      "1179725/1179725 [==============================] - 9s 7us/sample - loss: 0.1187 - accuracy: 0.9721 - val_loss: 0.1272 - val_accuracy: 0.9709\n",
      "Epoch 4/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1185 - accuracy: 0.9721 - val_loss: 0.1804 - val_accuracy: 0.9709\n",
      "Epoch 5/100\n",
      "1179648/1179725 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9721\n",
      "Epoch 00005: saving model to ./checkpoint/author.05-0.42.ckpt\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1175 - accuracy: 0.9721 - val_loss: 0.4209 - val_accuracy: 0.9646\n",
      "Epoch 6/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1171 - accuracy: 0.9723 - val_loss: 0.4108 - val_accuracy: 0.9710\n",
      "Epoch 7/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1163 - accuracy: 0.9724 - val_loss: 0.1731 - val_accuracy: 0.9716\n",
      "Epoch 8/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1157 - accuracy: 0.9724 - val_loss: 0.1200 - val_accuracy: 0.9716\n",
      "Epoch 9/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1159 - accuracy: 0.9724 - val_loss: 0.1197 - val_accuracy: 0.9716\n",
      "Epoch 10/100\n",
      "1179648/1179725 [============================>.] - ETA: 0s - loss: 0.1158 - accuracy: 0.9724\n",
      "Epoch 00010: saving model to ./checkpoint/author.10-0.12.ckpt\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1158 - accuracy: 0.9724 - val_loss: 0.1193 - val_accuracy: 0.9716\n",
      "Epoch 11/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1156 - accuracy: 0.9724 - val_loss: 0.1175 - val_accuracy: 0.9716\n",
      "Epoch 12/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1156 - accuracy: 0.9724 - val_loss: 0.1169 - val_accuracy: 0.9716\n",
      "Epoch 13/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1155 - accuracy: 0.9725 - val_loss: 0.1177 - val_accuracy: 0.9715\n",
      "Epoch 14/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1154 - accuracy: 0.9726 - val_loss: 0.1233 - val_accuracy: 0.9720\n",
      "Epoch 15/100\n",
      "1179648/1179725 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9725\n",
      "Epoch 00015: saving model to ./checkpoint/author.15-0.12.ckpt\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1155 - accuracy: 0.9725 - val_loss: 0.1169 - val_accuracy: 0.9716\n",
      "Epoch 16/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1151 - accuracy: 0.9726 - val_loss: 0.1171 - val_accuracy: 0.9721\n",
      "Epoch 17/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1150 - accuracy: 0.9727 - val_loss: 0.1164 - val_accuracy: 0.9722\n",
      "Epoch 18/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1149 - accuracy: 0.9727 - val_loss: 0.1162 - val_accuracy: 0.9723\n",
      "Epoch 19/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1149 - accuracy: 0.9727 - val_loss: 0.1172 - val_accuracy: 0.9721\n",
      "Epoch 20/100\n",
      "1179648/1179725 [============================>.] - ETA: 0s - loss: 0.1149 - accuracy: 0.9727\n",
      "Epoch 00020: saving model to ./checkpoint/author.20-0.12.ckpt\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1149 - accuracy: 0.9727 - val_loss: 0.1162 - val_accuracy: 0.9724\n",
      "Epoch 21/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1149 - accuracy: 0.9727 - val_loss: 0.1161 - val_accuracy: 0.9724\n",
      "Epoch 22/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1149 - accuracy: 0.9727 - val_loss: 0.1159 - val_accuracy: 0.9723\n",
      "Epoch 23/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1149 - accuracy: 0.9727 - val_loss: 0.1165 - val_accuracy: 0.9726\n",
      "Epoch 24/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1149 - accuracy: 0.9728 - val_loss: 0.1161 - val_accuracy: 0.9723\n",
      "Epoch 25/100\n",
      "1179648/1179725 [============================>.] - ETA: 0s - loss: 0.1149 - accuracy: 0.9728\n",
      "Epoch 00025: saving model to ./checkpoint/author.25-0.12.ckpt\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1149 - accuracy: 0.9728 - val_loss: 0.1159 - val_accuracy: 0.9725\n",
      "Epoch 26/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1148 - accuracy: 0.9728 - val_loss: 0.1158 - val_accuracy: 0.9726\n",
      "Epoch 27/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1148 - accuracy: 0.9728 - val_loss: 0.1158 - val_accuracy: 0.9726\n",
      "Epoch 28/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1148 - accuracy: 0.9728 - val_loss: 0.1157 - val_accuracy: 0.9726\n",
      "Epoch 29/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1148 - accuracy: 0.9728 - val_loss: 0.1160 - val_accuracy: 0.9726\n",
      "Epoch 30/100\n",
      "1179648/1179725 [============================>.] - ETA: 0s - loss: 0.1148 - accuracy: 0.9728\n",
      "Epoch 00030: saving model to ./checkpoint/author.30-0.12.ckpt\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1148 - accuracy: 0.9728 - val_loss: 0.1159 - val_accuracy: 0.9726\n",
      "Epoch 31/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1148 - accuracy: 0.9728 - val_loss: 0.1158 - val_accuracy: 0.9726\n",
      "Epoch 32/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1148 - accuracy: 0.9728 - val_loss: 0.1158 - val_accuracy: 0.9726\n",
      "Epoch 33/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1148 - accuracy: 0.9728 - val_loss: 0.1159 - val_accuracy: 0.9727\n",
      "Epoch 34/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1148 - accuracy: 0.9728 - val_loss: 0.1158 - val_accuracy: 0.9726\n",
      "Epoch 35/100\n",
      "1179648/1179725 [============================>.] - ETA: 0s - loss: 0.1148 - accuracy: 0.9728\n",
      "Epoch 00035: saving model to ./checkpoint/author.35-0.12.ckpt\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1148 - accuracy: 0.9728 - val_loss: 0.1160 - val_accuracy: 0.9725\n",
      "Epoch 36/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1147 - accuracy: 0.9728 - val_loss: 0.1158 - val_accuracy: 0.9726\n",
      "Epoch 37/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1147 - accuracy: 0.9728 - val_loss: 0.1159 - val_accuracy: 0.9726\n",
      "Epoch 38/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1147 - accuracy: 0.9728 - val_loss: 0.1159 - val_accuracy: 0.9726\n",
      "Epoch 39/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1147 - accuracy: 0.9728 - val_loss: 0.1158 - val_accuracy: 0.9726\n",
      "Epoch 40/100\n",
      "1179648/1179725 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9728\n",
      "Epoch 00040: saving model to ./checkpoint/author.40-0.12.ckpt\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1147 - accuracy: 0.9728 - val_loss: 0.1159 - val_accuracy: 0.9726\n",
      "Epoch 41/100\n",
      "1179725/1179725 [==============================] - 8s 7us/sample - loss: 0.1147 - accuracy: 0.9728 - val_loss: 0.1160 - val_accuracy: 0.9726\n",
      "Epoch 42/100\n",
      " 548864/1179725 [============>.................] - ETA: 4s - loss: 0.1140 - accuracy: 0.9729WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-964d25c00791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                             verbose=1) \n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=None\n",
    "train_ins = []\n",
    "for ins in pos_features:\n",
    "    train_ins.append((ins, 1))\n",
    "\n",
    "for ins in neg_features:\n",
    "    train_ins.append((ins, 0))\n",
    "\n",
    "print(np.array(train_ins).shape)\n",
    "\n",
    "random.shuffle(train_ins)\n",
    "\n",
    "x_train= []\n",
    "y_train = []\n",
    "for ins in train_ins:\n",
    "    x_train.append(ins[0])\n",
    "    y_train.append(ins[1])\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder=OneHotEncoder(sparse=False)\n",
    "yy=[[0],[1]]\n",
    "encoder.fit(yy)\n",
    "y_train_reshape=np.array(y_train).reshape(-1,1)\n",
    "y_train_onehot=encoder.transform(y_train_reshape)\n",
    "print(y_train_onehot[1:10])\n",
    "\n",
    "# y_test_reshape=np.array(y_test).reshape(-1,1)\n",
    "# y_test_onehot=encoder.transform(y_test_reshape)\n",
    "# print(y_test_onehot[1:10]\n",
    "x_train=np.array(x_train)\n",
    "import tensorflow as tf\n",
    "import pickle as p\n",
    "import os\n",
    "logdir=\"logs\"\n",
    "checkpoint_path='./checkpoint/author.{epoch:02d}-{val_loss:.2f}.ckpt'\n",
    "callbacks=[\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir,\n",
    "                                   histogram_freq=2),  # 生成tb需要的日志\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                      save_weights_only=True,\n",
    "                                      verbose=1,\n",
    "                                      period=5),  # 用于在epoch间保存要模型\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                         factor=0.1, \n",
    "                                         patience=3, \n",
    "                                         verbose=0, \n",
    "                                         mode='auto', \n",
    "                                         epsilon=0.0001, \n",
    "                                         cooldown=0, \n",
    "                                         min_lr=0),  # 当指标变化小时，减少学习率\n",
    "]\n",
    "checkpoint_dir=os.path.dirname(checkpoint_path)\n",
    "latest=tf.train.latest_checkpoint(checkpoint_dir)\n",
    "#建立Keras序列模型\n",
    "model=tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(units=512,\n",
    "                               input_dim=10,\n",
    "                               use_bias=True,\n",
    "                               kernel_initializer='uniform',\n",
    "                               bias_initializer='zeros',\n",
    "                               activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(units=256,\n",
    "                               activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(units=128,\n",
    "                               activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(units=64,\n",
    "                               activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(units=32,\n",
    "                               activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(units=2,\n",
    "                                activation='softmax'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.1),    \n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "#if latest:\n",
    "#    model.load_weights(latest)\n",
    "if train:    \n",
    "    train_history=model.fit(x=x_train,\n",
    "                            shuffle=True,\n",
    "                            y=y_train_onehot,\n",
    "                            validation_split=0.01,  # 0.2用作验证集\n",
    "                            epochs=100,\n",
    "                            batch_size=8192,\n",
    "                            callbacks=callbacks,\n",
    "                            verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集中的作者论文信息\n",
    "with open(\"cna_data/whole_author_profile.json\", \"r\") as f2:\n",
    "    test_author_data = json.load(f2)\n",
    "\n",
    "# 训练集的论文元信息\n",
    "with open(\"cna_data/whole_author_profile_pub.json\", \"r\") as f2:\n",
    "    test_pubs_dict = json.load(f2)\n",
    "\n",
    "# 待分配论文集\n",
    "with open(\"cna_test_data/cna_test_unass_competition.json\", \"r\") as f2:\n",
    "    unass_papers = json.load(f2)\n",
    "\n",
    "with open(\"cna_test_data/cna_test_pub.json\", \"r\") as f2:\n",
    "    unass_papers_dict = json.load(f2)\n",
    "name_list=[]\n",
    "# with open(\"cna_data/new_test_author_data.json\", 'r') as files:\n",
    "#     new_test_author_data = json.load(files)\n",
    "# 简单处理whole_author_profile，将同名的作者合并：\n",
    "# 为了效率，预处理new_test_author_data中的paper，将其全部处理成paper_id + '-' + author_index的形式。\n",
    "new_test_author_data = {}\n",
    "for author_id, author_info in tqdm.tqdm(test_author_data.items()):\n",
    "    author_name = author_info['name']\n",
    "    author_papers = author_info['papers']\n",
    "    newly_papers = []\n",
    "\n",
    "    for paper_id in author_papers:\n",
    "\n",
    "        paper_authors = test_pubs_dict[paper_id]['authors']\n",
    "        paper_authors_len = len(paper_authors)\n",
    "        \n",
    "        # 只利用author数小于50的paper，以保证效率\n",
    "        if(paper_authors_len > 50):\n",
    "            continue\n",
    "#         paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "        author_list = []\n",
    "        for author in paper_authors:                \n",
    "            clean_author = clean_name(author['name'])\n",
    "            if(clean_author != None):\n",
    "                author_list.append(clean_author)\n",
    "        if(len(author_list) > 0):\n",
    "            # 获取paper中main author_name所对应的位置\n",
    "            _, author_index = delete_main_name(author_list, author_name)\n",
    "#             print(paper_name)\n",
    "            new_paper_id = str(paper_id) + '-' + str(author_index)\n",
    "            newly_papers.append(new_paper_id)\n",
    "            \n",
    "       \n",
    "        \n",
    "        \n",
    "    if(new_test_author_data.get(author_name) != None):\n",
    "        new_test_author_data[author_name][author_id] = newly_papers\n",
    "    else:\n",
    "        tmp = {}\n",
    "        tmp[author_id] = newly_papers\n",
    "        new_test_author_data[author_name] = tmp\n",
    "        name_list.append(author_name)\n",
    "print(len(new_test_author_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test集的特征生成函数，与train类似\n",
    "def process_test_feature_coauthor(pair, new_test_author_data, test_pubs_dict, paper_coauthors):\n",
    "    \n",
    "    feature_list = []\n",
    "\n",
    "    paper = pair[0] \n",
    "    author = pair[1]\n",
    "    paper_name = pair[2]\n",
    "    \n",
    "    doc_list = new_test_author_data[paper_name][author]\n",
    "\n",
    "    \n",
    "    # 保存作者的所有coauthors以及各自出现的次数(作者所拥有论文的coauthors)\n",
    "    candidate_authors_int = defaultdict(int)\n",
    "\n",
    "    total_author_count = 0\n",
    "    for doc in doc_list:\n",
    "        doc_id = doc.split('-')[0]\n",
    "        author_index = doc.split('-')[1]\n",
    "        doc_dict = test_pubs_dict[doc_id]\n",
    "        author_list = []\n",
    "\n",
    "        paper_authors = doc_dict['authors']\n",
    "        paper_authors_len = len(paper_authors)\n",
    "        paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "    \n",
    "        for author in paper_authors:                \n",
    "            clean_author = clean_name(author['name'])\n",
    "            if(clean_author != None):\n",
    "                author_list.append(clean_author)\n",
    "        if(len(author_list) > 0):\n",
    "\n",
    "            # 获取除了main author_name外的coauthor\n",
    "            for index in range(len(author_list)):\n",
    "                if(index == author_index):\n",
    "                    continue\n",
    "                else:\n",
    "                    candidate_authors_int[author_list[index]] += 1\n",
    "                    total_author_count += 1\n",
    "\n",
    "    author_keys = list(candidate_authors_int.keys())\n",
    "\n",
    "    if ((len(author_keys) == 0) or (len(paper_coauthors) == 0)):\n",
    "        feature_list.extend([0.] * 5)\n",
    "    else:\n",
    "        co_coauthors = set(paper_coauthors) & set(author_keys)\n",
    "        coauthor_len = len(co_coauthors)\n",
    "        \n",
    "        co_coauthors_ratio_for_paper = round(coauthor_len / len(paper_coauthors), 6)\n",
    "        co_coauthors_ratio_for_author = round(coauthor_len / len(author_keys), 6)\n",
    "        \n",
    "        coauthor_count = 0\n",
    "        for coauthor_name in co_coauthors:\n",
    "            coauthor_count += candidate_authors_int[coauthor_name]\n",
    "            \n",
    "        \n",
    "        \n",
    "        co_coauthors_ratio_for_author_count = round(coauthor_count / total_author_count, 6)\n",
    "\n",
    "        # 计算了5维paper与author所有的paper的coauthor相关的特征：\n",
    "        #    1. 不重复的coauthor个数\n",
    "        #    2. 不重复的coauthor个数 / paper的所有coauthor的个数\n",
    "        #    3. 不重复的coauthor个数 / author的所有paper不重复coauthor的个数\n",
    "        #    4. coauthor个数（含重复）\n",
    "        #    4. coauthor个数（含重复）/ author的所有paper的coauthor的个数（含重复）\n",
    "        feature_list.extend([coauthor_len, co_coauthors_ratio_for_paper, co_coauthors_ratio_for_author, coauthor_count, co_coauthors_ratio_for_author_count])\n",
    "        \n",
    "#         print(feature_list)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test集的特征生成函数，与train类似\n",
    "def process_test_feature_keywords(pair, new_test_author_data, test_pubs_dict, paper_coauthors):\n",
    "    \n",
    "    feature_list = []\n",
    "\n",
    "    paper = pair[0] \n",
    "    author = pair[1]\n",
    "    paper_name = pair[2]\n",
    "    \n",
    "    doc_list = new_test_author_data[paper_name][author]\n",
    "\n",
    "    \n",
    "    # 保存作者的所有coauthors以及各自出现的次数(作者所拥有论文的coauthors)\n",
    "    candidate_authors_int = defaultdict(int)\n",
    "\n",
    "    total_author_count = 0\n",
    "    for doc in doc_list:\n",
    "        doc_id = doc.split('-')[0]\n",
    "        author_index = doc.split('-')[1]\n",
    "        doc_dict = test_pubs_dict[doc_id]\n",
    "        author_list = []\n",
    "        \n",
    "        if 'keywords' not in doc_dict.keys():\n",
    "            continue\n",
    "        paper_authors = doc_dict['keywords']\n",
    "        paper_authors_len = len(paper_authors)\n",
    "        paper_authors = random.sample(paper_authors, min(10, paper_authors_len))\n",
    "    \n",
    "        for author in paper_authors:                \n",
    "            author_list.append(author)\n",
    "        if(len(author_list) > 0):\n",
    "\n",
    "            # 获取除了main author_name外的coauthor\n",
    "            for index in range(len(author_list)):\n",
    "                if(index == author_index):\n",
    "                    continue\n",
    "                else:\n",
    "                    candidate_authors_int[author_list[index]] += 1\n",
    "                    total_author_count += 1\n",
    "    author_keys = list(candidate_authors_int.keys())\n",
    "\n",
    "    if ((len(author_keys) == 0) or (len(paper_coauthors) == 0)):\n",
    "        feature_list.extend([0.] * 5)\n",
    "    else:\n",
    "        co_coauthors = set(paper_coauthors) & set(author_keys)\n",
    "        coauthor_len = len(co_coauthors)\n",
    "#         print(coauthor_len)\n",
    "        co_coauthors_ratio_for_paper = round(coauthor_len / len(paper_coauthors), 6)\n",
    "        co_coauthors_ratio_for_author = round(coauthor_len / len(author_keys), 6)\n",
    "        \n",
    "        coauthor_count = 0\n",
    "        for coauthor_name in co_coauthors:\n",
    "            coauthor_count += candidate_authors_int[coauthor_name]\n",
    "            \n",
    "        \n",
    "        \n",
    "        co_coauthors_ratio_for_author_count = round(coauthor_count / total_author_count, 6)\n",
    "\n",
    "        # 计算了5维paper与author所有的paper的coauthor相关的特征：\n",
    "        #    1. 不重复的coauthor个数\n",
    "        #    2. 不重复的coauthor个数 / paper的所有coauthor的个数\n",
    "        #    3. 不重复的coauthor个数 / author的所有paper不重复coauthor的个数\n",
    "        #    4. coauthor个数（含重复）\n",
    "        #    4. coauthor个数（含重复）/ author的所有paper的coauthor的个数（含重复）\n",
    "        feature_list.extend([coauthor_len, co_coauthors_ratio_for_paper, co_coauthors_ratio_for_author, coauthor_count, co_coauthors_ratio_for_author_count])\n",
    "        \n",
    "#         print(feature_list)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unass_papers))\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "# 存储paper的所有candidate author id\n",
    "paper2candidates = defaultdict(list)\n",
    "# 存储对应的paper与candidate author的生成特征\n",
    "paper2features = defaultdict(list)\n",
    "\n",
    "for u_p in tqdm.tqdm(unass_papers):\n",
    "    paper_id = u_p.split('-')[0]\n",
    "    author_index = int(u_p.split('-')[1])\n",
    "    author_list = []\n",
    "    \n",
    "    # 获取paper的coauthors\n",
    "    paper_coauthors = []\n",
    "    keywords_list=[]\n",
    "    paper_name = ''\n",
    "    paper_authors = unass_papers_dict[paper_id]['authors']\n",
    "    if 'keywords' in unass_papers_dict[paper_id].keys():\n",
    "        paper_keywords=unass_papers_dict[paper_id]['keywords']\n",
    "    else:\n",
    "        paper_keywords=''\n",
    "    author_name=''\n",
    "    org=''\n",
    "#     paper_authors_len = len(paper_authors)\n",
    "#     paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "\n",
    "    for author in paper_authors:                \n",
    "        clean_author = clean_name(author['name'])\n",
    "        if(clean_author != None):\n",
    "            author_list.append(clean_author)\n",
    "    for key in paper_keywords:\n",
    "        clean_key=clean_name(key)\n",
    "        if clean_key!=None:\n",
    "            keywords_list.append(clean_key)\n",
    "    if(len(author_list) > 0):\n",
    "        \n",
    "        # 获取除了main author_name外的coauthor\n",
    "        for index in range(len(author_list)):\n",
    "            if(index == author_index):\n",
    "                author_name=author_list[author_index]\n",
    "                continue\n",
    "            else:\n",
    "                paper_coauthors.append(author_list[index])\n",
    "    for author in paper_authors:\n",
    "#         print(author.keys())\n",
    "        clean_author = clean_name(author['name'])\n",
    "        \n",
    "        if author_name==clean_author and 'org' in author.keys():\n",
    "            org=author['org']\n",
    "#             print('in')\n",
    "            \n",
    "# 简单使用精确匹配找出candidate_author_list\n",
    "   \n",
    "\n",
    "    if paper_authors[author_index]['name'] == None or paper_authors[author_index]['name']==' ':\n",
    "        paper_name=' '\n",
    "    else:\n",
    "        paper_name = '_'.join(clean_name(paper_authors[author_index]['name']).split())\n",
    "    if(new_test_author_data.get(paper_name) != None):\n",
    "        candidate_author_list = new_test_author_data[paper_name]\n",
    "        for candidate_author in candidate_author_list:\n",
    "            pair = (paper_id, candidate_author, paper_name)\n",
    "            paper2candidates[paper_id].append(candidate_author)\n",
    "            paper2features[paper_id].append(process_test_feature_coauthor(pair, new_test_author_data, test_pubs_dict, paper_coauthors)+\n",
    "                                            process_test_feature_keywords(pair, new_test_author_data, test_pubs_dict, paper_keywords))\n",
    "        count += 1\n",
    "    else:\n",
    "        score=0.0\n",
    "        name_index=' '\n",
    "        for name in (name_list):\n",
    "            name_split=name.split('_')\n",
    "            temp = distance.get_jaro_distance(paper_name, name, winkler=True, scaling=0.1)\n",
    "            pname_split = paper_name.split('_')\n",
    "            inter = set(name_split) & set(pname_split)\n",
    "            alls = set(name_split) | set(pname_split)\n",
    "            temp += round(len(inter)/len(alls), 6)\n",
    "            if score<temp:\n",
    "                name_index=name\n",
    "                score=temp\n",
    "        if(new_test_author_data.get(name_index) != None):\n",
    "            candidate_author_list = new_test_author_data[name_index]\n",
    "            for candidate_author in candidate_author_list:\n",
    "                pair = (paper_id, candidate_author, name_index)\n",
    "                paper2candidates[paper_id].append(candidate_author)\n",
    "                paper2features[paper_id].append(process_test_feature_coauthor(pair, new_test_author_data, test_pubs_dict, paper_coauthors)+\n",
    "                                            process_test_feature_keywords(pair, new_test_author_data, test_pubs_dict, paper_keywords))\n",
    "                \n",
    "            count+=1\n",
    "            \n",
    "            \n",
    "print(count)\n",
    "assert len(paper2candidates) == len(paper2features)\n",
    "print(len(paper2candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(list)\n",
    "for paper_id, ins_feature_list in tqdm.tqdm(paper2features.items()): \n",
    "    score_list = []\n",
    "    prob_pred = model.predict(np.array(ins_feature_list))[:, 1]\n",
    "    score_list.extend(prob_pred)\n",
    "#     for ins in ins_feature_list:\n",
    "#         # 利用svm对一篇paper的所有candidate author去打分，利用分数进行排序，取top-1 author作为预测的author\n",
    "# #         print(ins)\n",
    "#         prob_pred = model.predict(np.array([ins]))[:, 1]\n",
    "#         score_list.append(prob_pred[0])\n",
    "    rank = np.argsort(-np.array(score_list))\n",
    "    #取top-1 author作为预测的author\n",
    "    predict_author = paper2candidates[paper_id][rank[0]]\n",
    "    result_dict[predict_author].append(paper_id)\n",
    "\n",
    "with open(\"cna_data/result_dnn.json\", 'w') as files:\n",
    "    json.dump(result_dict, files, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cna_data/paper2features.json\", 'w') as files:\n",
    "    json.dump(result_dict, files, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(list)\n",
    "for paper_id, ins_feature_list in tqdm.tqdm(paper2features.items()): \n",
    "    score_list = []\n",
    "    for ins in ins_feature_list:\n",
    "        # 利用svm对一篇paper的所有candidate author去打分，利用分数进行排序，取top-1 author作为预测的author\n",
    "#         print(ins)\n",
    "#         print([ins])\n",
    "        prob_pred = clf.predict_proba([ins])[:, 1]\n",
    "        score_list.append(prob_pred[0])\n",
    "    rank = np.argsort(-np.array(score_list))\n",
    "    #取top-1 author作为预测的author\n",
    "    predict_author = paper2candidates[paper_id][rank[0]]\n",
    "    result_dict[predict_author].append(paper_id)\n",
    "\n",
    "with open(\"cna_data/result_svm.json\", 'w') as files:\n",
    "    json.dump(result_dict, files, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
